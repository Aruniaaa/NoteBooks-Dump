{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyM4iPe9Ew6of4jiPPopqMiF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Aruniaaa/NoteBooks-Dump/blob/main/GPT_from_scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Building an GPT from scratch\n",
        "\n",
        "Tutorial used - [Andrej Karpathy's](https://youtu.be/kCc8FmEb1nY?si=J9uV8cDOVSjrIds6)\n",
        "\n",
        "GPT - Generative Pre-Trained Transformer\n",
        "\n",
        "Basically, the following code is a way to build Neural Networks/Transformers/AI models such that it can accurately predict what comes next in a sentence or collection of words, given some collection of word (training data)"
      ],
      "metadata": {
        "id": "0H-5lQnX_AGz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "path = '/content/drive/MyDrive/Datasets/Del-data.txt' # your path will certainly be different\n",
        "\n",
        "with open(path, \"r\") as file:\n",
        "  data = file.read()\n"
      ],
      "metadata": {
        "id": "rzl2skQ5_GYu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7fb62b2a-4b57-467d-bc4f-359ac40c76fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Basics\n",
        "\n",
        "What I'm doing here:\n",
        "1. Storing all the unique characaters in the dataset into a list called \"chars\". The size of \"chars\" is our vocab size which is stored in the variable \"size\"\n",
        "\n",
        "2. Mapping each individual and unique character in the dataset to a number for tokenization (Example -> a = 18, r = 33, p = 32). The mapping is stored in the form of a dictionary \"stoi\".\n",
        "\n",
        "3. Reversing that mapping so that every integer corresponds with a letter or character, this is stored in the dictionary \"itos\"\n",
        "\n",
        "4. The encoder and decoder are lambda functions that get the string to integer or integer to string conversion and join it together in one array/string\n",
        "\n"
      ],
      "metadata": {
        "id": "iV7BMXsTiyV2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chars = sorted(list(set(data)))\n",
        "size = len(chars)\n",
        "\n",
        "stoi = {ch: i for i, ch in enumerate(chars)}\n",
        "itos = {i : ch for i, ch in enumerate(chars)}\n",
        "\n",
        "encode = lambda sentence: [stoi[c] for c in sentence ]\n",
        "decode = lambda arr: ''.join([itos[i] for i in arr])\n"
      ],
      "metadata": {
        "id": "4BWVOVQEImT7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F"
      ],
      "metadata": {
        "id": "WQSKiqpfJRXh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Splitting and Batch Processing\n",
        "\n",
        "The following two cells do the below mentioned:\n",
        "\n",
        "1. Creating a torch.tensor object (you can think of this as a list/array) that stores the encoding of the data into integers. So, each character in the data, regardless of whether its unique is encoded into the integer from the stoi dictionary (if r = 33 in stoi, every occurence of r in the data will be 33) and stored in the new data variable. Think of it as writing the whole Del-data.txt file, but in the form of numbers\n",
        "\n",
        "2. We are then taking the number which is 90% of the length of our data's size and storing it into the variable n, which we use to split our data into train and test. Learn more about train test split [here](https://builtin.com/data-science/train-test-split)\n",
        "\n",
        "3. Next we define a get batch function that randomly grabs [batch size] sequences of data, each sequence containing [block size] characters. The split arguement is passed into the function so that x and y are retrieved from the correct dataset.\n",
        "For example, if our model is in the training mode we would not want to get x and y (context and targets) from the test split we defined in the below cell, and vice versa.\n",
        "\n",
        "4. ix is a list of [batch size] amount of starting points for our x and y values, the len(data) - block size ensures that we don't grab starting positions from the data so close to the ending that we can't even travel [block size] amount of steps ahead.\n",
        "\n",
        "5. x and y here are arrays of [batch size] by [block size], each row has [block size] sequential elements encoded into integers. For example, if the 0th integer in ix (a random starting point) was 46, the first row of x would have all the characters including the 46th to 46 + [block size]th element of the train/test data. (In this case that would be data[46 : 54])\n",
        "\n",
        "6. Every element in y is the same as x, its just offset by 1 to the right so that y is a vector with each element corresponding to the target integer for the same element in the x vector (You can run the for loop below to better understand this)\n"
      ],
      "metadata": {
        "id": "qFWKBJow4ejj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = torch.tensor(encode(data), dtype=torch.long)\n",
        "\n",
        "n = int(0.9 * len(data))\n",
        "train = data[: n]\n",
        "test = data[n : ]"
      ],
      "metadata": {
        "id": "V361BNJMIs_l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 4 # how many independent sequences will we process in parallel?\n",
        "block_size = 8 # what is the maximum context length for predictions?\n",
        "\n",
        "def get_batch(split):\n",
        "    # generate a small batch of data of inputs x and targets y\n",
        "    data = train if split.lower() == 'train' else test\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    return x, y\n",
        "\n",
        "xb, yb = get_batch('train')\n",
        "print('inputs:')\n",
        "print(xb.shape)\n",
        "print(xb)\n",
        "print('targets:')\n",
        "print(yb.shape)\n",
        "print(yb)\n",
        "\n",
        "print('----')\n",
        "\n",
        "for b in range(batch_size): # batch dimension\n",
        "    for t in range(block_size): # time dimension\n",
        "        context = xb[b, :t+1]\n",
        "        target = yb[b,t]\n",
        "        print(f\"when input is {context.tolist()} the target: {target}\")"
      ],
      "metadata": {
        "id": "YWN-VuilPFC6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c6136e9c-913b-4c0d-b9bd-678c9d399a05"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inputs:\n",
            "torch.Size([4, 8])\n",
            "tensor([[30, 21,  1, 35, 25, 22,  1, 29],\n",
            "        [20, 22,  2,  1, 34, 35, 18, 19],\n",
            "        [ 1, 21, 22, 28, 22, 24, 18, 35],\n",
            "        [27,  1, 40, 31, 36,  2,  1,  8]])\n",
            "targets:\n",
            "torch.Size([4, 8])\n",
            "tensor([[21,  1, 35, 25, 22,  1, 29, 31],\n",
            "        [22,  2,  1, 34, 35, 18, 19, 26],\n",
            "        [21, 22, 28, 22, 24, 18, 35, 26],\n",
            "        [ 1, 40, 31, 36,  2,  1,  8, 31]])\n",
            "----\n",
            "when input is [30] the target: 21\n",
            "when input is [30, 21] the target: 1\n",
            "when input is [30, 21, 1] the target: 35\n",
            "when input is [30, 21, 1, 35] the target: 25\n",
            "when input is [30, 21, 1, 35, 25] the target: 22\n",
            "when input is [30, 21, 1, 35, 25, 22] the target: 1\n",
            "when input is [30, 21, 1, 35, 25, 22, 1] the target: 29\n",
            "when input is [30, 21, 1, 35, 25, 22, 1, 29] the target: 31\n",
            "when input is [20] the target: 22\n",
            "when input is [20, 22] the target: 2\n",
            "when input is [20, 22, 2] the target: 1\n",
            "when input is [20, 22, 2, 1] the target: 34\n",
            "when input is [20, 22, 2, 1, 34] the target: 35\n",
            "when input is [20, 22, 2, 1, 34, 35] the target: 18\n",
            "when input is [20, 22, 2, 1, 34, 35, 18] the target: 19\n",
            "when input is [20, 22, 2, 1, 34, 35, 18, 19] the target: 26\n",
            "when input is [1] the target: 21\n",
            "when input is [1, 21] the target: 22\n",
            "when input is [1, 21, 22] the target: 28\n",
            "when input is [1, 21, 22, 28] the target: 22\n",
            "when input is [1, 21, 22, 28, 22] the target: 24\n",
            "when input is [1, 21, 22, 28, 22, 24] the target: 18\n",
            "when input is [1, 21, 22, 28, 22, 24, 18] the target: 35\n",
            "when input is [1, 21, 22, 28, 22, 24, 18, 35] the target: 26\n",
            "when input is [27] the target: 1\n",
            "when input is [27, 1] the target: 40\n",
            "when input is [27, 1, 40] the target: 31\n",
            "when input is [27, 1, 40, 31] the target: 36\n",
            "when input is [27, 1, 40, 31, 36] the target: 2\n",
            "when input is [27, 1, 40, 31, 36, 2] the target: 1\n",
            "when input is [27, 1, 40, 31, 36, 2, 1] the target: 8\n",
            "when input is [27, 1, 40, 31, 36, 2, 1, 8] the target: 31\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hyper Params\n",
        "\n",
        "\"In machine learning, a hyperparameter is a parameter that can be set in order to define any configurable part of a model's learning process\"\n",
        "\n",
        "Batch size = How many independent sequences will we process in parallel? You can also think of this as how many starting points we will have to create the batch of data. This is important as we can not pass the entire training data into the model at once since it will be evry computationally expensive\n",
        "\n",
        "Block size = Given we have [batch size] amount of sequences, how many characters should each sequence have? Here, we will have 16 independent sequences from the data, each of which will have 32 characters. For example, one of the sequences could be \"Distinguished delegates, I exten\", it has 32 characters.\n",
        "\n",
        "Max Iters (Iteration) = During training, how many times should the model be trained or allowed to see new data? For example, one iteration would mean that the entire training proccess (getting the x and y values, calclulating the loss, optimizing, and backward propogation) would happen one time. For 5000 iterations, the same proccess will repeat 5000 times.\n",
        "\n",
        "Eval (evaluation) interval = During training and testing, I want to show the user how the loss is changing for debugging proccess or just to visually see the process and keep getting updates in the long period of training + testing. The eval interval will be used to print the loss during training and testing during every [eval_interval] intervals.\n",
        "\n",
        "Device = Since there will be a LOT of matrix multiplications and math involved, we're better off using the GPU if possible (which can perform upto 36 trillion computations per second) rather than the CPU, which is relatively slow. ***THIS LINE IS NOT NEEDED FOR GOOGLE COLAB***\n",
        "\n",
        "Eval_iters = The amount of items we will get the losses for. Instead of just calcualting the loss on the 16 batch size dataset one time, we will calculate the losses [eval_iters] times and then calculate the average\n",
        "\n",
        "N_embed = You're gonna need some linear algebra for this one. It Basically creates a 64 dimension vector space where each vector represents a character. So, the letter h is represented by a vector of 64 real numbers where each dimension stores something about the character. Dimension 21 might represent \"frequency in English text\", dimension 47 might represent \"likelihood of appearing after 'q'\", etc\n",
        "\n",
        "n_head = This is the basic for our multiheaded attention mechanism. The 64-dimensional embedding gets split into four chunks of 16 dimensions each (64 / 4 = 16). Each attention head operates independently on its 16-dimensional slice, learning its own Query, Key, and Value transformations. One chunk might focus on short term relationship, the other might focus on positional relationships, etc\n",
        "\n",
        "n_layer = Now, information from the 4 different chunks go to a different layer where it is then refined and improved, and this n_layer is the amount of timed we do this. Each layer builds on the information from the one before it, so by the time the data gets to the last layer, the model has a very deep understanding of the relationships in the data. With four layers, the information goes through four rounds of processing and refinement.\n",
        "\n",
        "Dropout = To prevent [overfitting](https://www.ibm.com/think/topics/overfitting), randomly during the training, some of the neurons \"shut down\", in a way that they don't participate in the prediction process or to pass along any information. This is done so that the model doesn't become too dependant on any one neuron or set of neurons. Here the dropout is 0.0, which means all the neurons will be active, we will tweak this value later."
      ],
      "metadata": {
        "id": "CWMiz-n4574t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 16\n",
        "block_size = 32\n",
        "max_iters = 5000\n",
        "eval_interval = 100\n",
        "learning_rate = 1e-3\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "n_embed = 64\n",
        "n_head = 4\n",
        "n_layer = 4\n",
        "dropout = 0.0\n"
      ],
      "metadata": {
        "id": "T3WPuW4c563m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Estimating the Loss\n",
        "\n",
        "Loss is basically a number evaluating the performance of a model on both the training and validation datasets. It tells you how well a model is doing on both the training and testing datasets\n",
        "\n",
        "\n",
        "@torch.no_grad() = This is a PyTorch context manager that disables gradient calculation. During the forward pass, PyTorch's autograd engine normally builds a computation graph to track operations, which is required for backpropagation and weight updates. By wrapping the evaluation logic in @torch.no_grad(), we prevent this graph from being built.\n",
        "\n",
        "model.eval() = sets the model to evaluation mode. This is important because certain layers, like Dropout and Batch Normalization, behave differently during training and evaluation. For example in evaluation, all neurons are active and the dropout is set to 0\n",
        "\n",
        "we iterate over two different splits, 'train' which tells us how the model is performing on the training dataset, i.e, the data it has seen before, and the testing dataset, i.e, data it has not seen before.\n",
        "\n",
        "Then, we get X and Y and pass it into our model to make predictions which returns logits (the actual prediction) and the loss (how well it performed)\n",
        "\n",
        "We average out the losses (for both the train and test data) and store it in the out dictionary. The model is then set back to training mode.\n",
        "\n"
      ],
      "metadata": {
        "id": "Je0vCrj1G6ZA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out"
      ],
      "metadata": {
        "id": "XaMMFT5v6ERv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transformer\n",
        "\n",
        "From here, we start building the actual transformer.\n"
      ],
      "metadata": {
        "id": "nqyM6Pf9K6IM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Attention Head\n",
        "\n",
        " It's basically a special mechanism that allows a model to weigh the importance of different words in a sequence when processing a single word. It looks at all the other words to figure out which ones are most relevant to the current word. This helps the model understand context. For example, in the sentence \"The dog chased the cat, and it ran away\", who does \"it\" refer to?\n",
        "\n",
        "\n",
        " Each token (a character) in the data/text has 3 vectors\n",
        "\n",
        " 1. Query vector (going to be represented as Q from now): This is like each token asking \"Are there any tokens that I should pay attention to?\" or \"I am looking for a token that is the character '.' and comes after the letter 't'\", etc etc\n",
        "\n",
        " 2. Key vector (going to be represented as Q from now): This is like each token saying \"I am h, a constant, at position 5, who is followed by the token 'e'\" or any other relevant info.\n",
        "\n",
        " Now the way that the attention works is by calculating the dot product of each query vector and each key vector. It's like comparing the Q of the current token with the K of all other tokens. The more similar the Q is to a K, the more attention the model pays to that token.\n",
        "\n",
        " wei = q @ k.transpose(-2,-1) means that for each token, it will create a matrix containing the attention scores.\n",
        "\n",
        " wei.masked_fill(...): ensures that a word can only pay attention to the words that came before it. This is important for language generation, as the model can't \"cheat\" by looking at future words.\n",
        "\n",
        " F.softmax(wei, dim=-1): The scores are then turned into probabilities, so they all add up to 1.\n",
        "\n",
        " out = wei @ v: Instead of each token just focusing on the most \"relevant\" one in the attention matrix, it pays attention to all the tokens, but differently. Instead of a token paying attention 100% to on one other token, it splits that up into different token where the most relevant token still gets most of the attention.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "eD5CBTjXMdgW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Head(nn.Module):\n",
        "\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(n_embed, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embed, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embed, head_size, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B,T,C = x.shape\n",
        "        k = self.key(x)\n",
        "        q = self.query(x)\n",
        "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
        "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
        "        wei = self.dropout(wei)\n",
        "        # perform the weighted aggregation of the values\n",
        "        v = self.value(x) # (B,T,C)\n",
        "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
        "        return out\n"
      ],
      "metadata": {
        "id": "HwticzewtFEE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Multiple Attention Heads\n",
        "\n",
        "Why do we need it?\n",
        "Well, one attention head works quite well, but this is a generalist vs specialist case. Would you rather have on attention head that tries to capture everything like which tokens it should pay attention to, what's the underlying relationships, frequency, etc.\n",
        "\n",
        "Or have a little team of those attention heads that each focus on a different thing, but they do it brilliantly?\n",
        "\n",
        "In the following code, [num_heads] heads are created which each creates a new representation of each token that reflects what that head learned to pay attention to.\n",
        "\n",
        "So, each head takes the input tensor x (batch_size, 32, 64) and creates a (batch_size, 32, 16) vector focusing on *one* aspect each instead of trying to generalize ALL the relationships in that 16 dimension space.\n",
        "\n",
        "For example,\n",
        "\n",
        "Head 1 produces (batch_size, 32, 16) focusing on, say, grammatical relationships\n",
        "Head 2 produces (batch_size, 32, 16) focusing on, say, semantic similarities\n",
        "Head 3 produces (batch_size, 32, 16) focusing on, say, positional patterns\n",
        "Head 4 produces (batch_size, 32, 16) focusing on, say, long-range dependencies\n",
        "\n",
        "```\n",
        "out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "```\n",
        " This gives us a 64-dimensional representation (4 heads × 16 dimensions) where each token now has multiple complementary perspectives encoded within it.\n",
        "\n",
        " Now, before feeding this vector into a different layer, we need to project it. The current structure is completely different from the original one. The original 64 dimension represented mixed information, while this one represents rigid and organised one.\n",
        "\n",
        " For example, the first 16 dimension can represent grammatical relationships, the second 16 will represent semantic relationships, etc etc.\n",
        "\n",
        " So, the projection transforms structured, specialized information back into a flexible, general representation that can be used by the next layer in the stack.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "4YcSc95iYw6J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadedAttention(nn.Module):\n",
        "\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "        self.proj = nn.Linear(n_embed, n_embed)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        out = self.dropout(self.proj(out))\n",
        "        return out\n"
      ],
      "metadata": {
        "id": "oXV6BnyVyHzA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Feed Forward\n",
        "\n",
        "Why do we need it? Think of it like this, the multi headed attention enabled the tokens to look at each other and communicate. Let's say we had the phrase \"A fluffy blue creature roams the verdant forest\". After multi headed attention, the token \"creature\" now knows its related to or should pay attention to the words \"blue\", \"fluffy\", and \"roams\", but it did not get a lot of time to actually *think* about it.\n",
        "\n",
        "Which means that the token does not actually know that its fluffy and blue and is roaming, it just knows it's supposed to pay a lot of attention to those tokens\n",
        "\n",
        "Now what this does is that it takes the output vector from the multi headed attention, and changes the dimensions from 64 to 256 (multiplies by 4), so as to have more \"thinking space\".\n",
        "\n",
        "This \"thinking space\" allows the token \"creature\" to now focus on WHY and HOW those attention scores matter. It learns patterns in the data like how an adjective is almost always followed by a noun, how a capitalization is almost always precceded by a fullstop, etc etc.\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        " nn.Linear(4 * n_embed, n_embed),\n",
        "```\n",
        "This line squishes the newly formed 256 dimension vector back into a 64 dimension one, keeping the most relevant parts in a concise manner, but this new vector now has all the \"thinking\" and understanding encoded within it.\n"
      ],
      "metadata": {
        "id": "qPsIq8_fhb8D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class FeedForward(nn.Module):\n",
        "\n",
        "    def __init__(self, n_embed):\n",
        "        super().__init__()\n",
        "        self.network = nn.Sequential(\n",
        "            nn.Linear(n_embed, n_embed * 4),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * n_embed, n_embed),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.network(x)\n",
        "\n"
      ],
      "metadata": {
        "id": "a_i8OTQp1Pl4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Blocks/Layers\n",
        "\n",
        "The job of the block is to understand complex relationships and patterns like \"this forms a complete word\", \"this is a verb\", \"full stop is used to end a sentence\", etc.\n",
        "\n",
        "It's different from multiple attention heads as multiattention heads realise the context within the input. For example, which letters go together most often, which one appears before a fullstop, etc etc.\n",
        "\n",
        "But Blocks/Layers BUILD contexts, realising how the words/letters fit together, when, where, how, and WHY. It understands that \"the cat\" are two words and need to be separated via space, etc etc.\n",
        "\n",
        "It also preserves information while adding new understanding. At each stage, the Block says \"keep everything you already knew, but now add this new layer of insight\" through the residual connections (the x = x + ... operations).\n",
        "\n",
        "And, the understanding from one layer passes onto the other to refine and improve!\n",
        "\n",
        "We use two layer norms here, the first one ensures that the data is properly scaled before moving into the multiheaded attention, the second one ensures\n",
        "\n",
        "The second layer takes the output from the attention stage and normalizes it across the embedding dimensions, ensuring that the feed-forward network receives inputs with stable, well-behaved statistical properties. This is important because the feed-forward network uses learned weight matrices, and these matrices work best when their inputs have consistent scaling and distribution. Without this normalization, the feed-forward network might struggle with inputs that are too large, too small, or have unusual distributions, potentially leading to training instability or poor performance."
      ],
      "metadata": {
        "id": "ygVj2LnkNwf9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class Block(nn.Module):\n",
        "\n",
        "    def __init__(self, n_embed, n_head):\n",
        "        super().__init__()\n",
        "        head_size = n_embed // n_head\n",
        "        self.sa = MultiHeadedAttention(n_head, head_size)\n",
        "        self.ffwd = FeedForward(n_embed)\n",
        "        self.ln1 = nn.LayerNorm(n_embed)\n",
        "        self.ln2 = nn.LayerNorm(n_embed)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.sa(self.ln1(x))\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "        return x"
      ],
      "metadata": {
        "id": "CnhJPv4D3jBL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The Actual Model\n",
        "\n",
        "Here, in the init we combine everything (attention heads, feed forward, blocks, position embedding, etc) to finally create the model. It's like taking the pieces of the jigsaw puzzles that we've been building and combining it. The combination is the model here.\n",
        "\n",
        "First, we add the n_embed dimensions and positional encoding to our (B,T) tensor by\n",
        "\n",
        "```\n",
        "x = tok_emb + pos_emb # (B,T,C)\n",
        "```\n",
        "The X tensor then flows to the multiple attention layers/blocks, the output X tensor with the same dimensions then gets normalized.\n",
        "\n",
        "The lm_head is a linear layer that takes the proccessed embeddings and generates the raw scores, called \"logits\" for every token in our vocabulary.\n",
        "\n",
        "Think of it like this,\n",
        "\n",
        "after all the transformer processing, each position has learned a rich representation that captures context like \"we're talking about animals, and the last word was 'the'.\" Now lm_head looks at that representation and says:\n",
        "\n",
        "\"cat\" gets a score of 8.2\n",
        "\"dog\" gets a score of 7.1\n",
        "\"pizza\" gets a score of 0.3\n",
        "\"the\" gets a score of -2.1\n",
        "\n",
        "The higher the score (logit), the more likely the model thinks that word should come next.\n",
        "\n",
        "The generate function ->\n",
        "\n",
        "It implements autoregressive sampling, which means that the model predicts one token at a time, then feeds that prediction back as input for the next prediction. This creates a feedback loop that can generate arbitrarily long sequences.\n",
        "\n",
        "The model can only have the previous block_size amount of context for each prediction, by the way, so, if block_size is 32 and we've generated 40 tokens so far, we only use the last 32 tokens as context for predicting token 41. This prevents memory issues and keeps inference fast.\n",
        "\n",
        "Since logits are just raw scores, we use softmax to convert it into a probability distribution.\n",
        "\n",
        "After that, we use torch.multinomial.\n",
        "\n",
        "Let's say we had the sentence \"The cat sat on the\", and we need to predict the next word. Now, chair could be the most probable, followed be table, bed, etc etc.\n",
        "\n",
        "Words like elephant or juice are examples of the least probable words.\n",
        "\n",
        "Now imagine a spinning wheel with those words. Chair makes up 50% of the wheel, table makes up 30%, bed makes up 10% elephant and juice make up 5% each. We spin the wheel. Now its most likely going to land on wheel, but it can even land on table, bed, even juice. The word it lands on is the predicted word.\n",
        "\n",
        "Why do we do this? Well, so that the model doesn't generate a fixed sequence of characters every time. Think about how boring it would be if ChatGPT or any other LLM only gave one, specific, fixed response to a specific prompts. To prevent this, and add a bit of \"randomness\", we \"spin the wheel\"\n",
        "\n",
        "We then concatenate the newly sampled token to our existing sequence. The sequence grows from length T to T+1. This becomes the input for the next iteration of the loop.\n",
        "\n",
        "The longer sequence (with our newly sampled token) becomes the input for the next forward pass. The model will use this extended context to predict the token that should come after our newly sampled token. And so on, and so on."
      ],
      "metadata": {
        "id": "YaIXwO4GV71g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "  def __init__(self, vocab_size):\n",
        "    super().__init__()\n",
        "    self.token_embedding_table = nn.Embedding(vocab_size, n_embed)\n",
        "    self.position_embedding_table = nn.Embedding(block_size, n_embed)\n",
        "    self.blocks = nn.Sequential(*[Block(n_embed, n_head=n_head) for _ in range(n_layer)])\n",
        "    self.ln_f = nn.LayerNorm(n_embed) # final layer norm\n",
        "    self.lm_head = nn.Linear(n_embed, vocab_size)\n",
        "\n",
        "\n",
        "  def forward(self, idx, targets=None):\n",
        "\n",
        "    idx = idx.to(device) # Move input tensor to the correct device\n",
        "\n",
        "    B, T = idx.shape\n",
        "\n",
        "    # idx and targets are both (B,T) tensor of integers\n",
        "    tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
        "    pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
        "    x = tok_emb + pos_emb # (B,T,C)\n",
        "    x = self.blocks(x) # (B,T,C)\n",
        "    x = self.ln_f(x) # (B,T,C)\n",
        "    logits = self.lm_head(x) # (B,T,vocab_size)\n",
        "    if targets is None:\n",
        "      loss = None\n",
        "    else:\n",
        "      targets = targets.to(device) # Move targets tensor to the correct device\n",
        "      B, T, C = logits.shape\n",
        "      logits = logits.view(B * T, C)\n",
        "      targets = targets.view(B * T)\n",
        "      loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "    return logits, loss\n",
        "\n",
        "  def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        idx = idx.to(device) # Move input tensor to the correct device\n",
        "        for _ in range(max_new_tokens):\n",
        "            # crop idx to the last block_size tokens\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx_cond)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx\n",
        "\n",
        "\n",
        "model = BigramLanguageModel(size)\n",
        "model.to(device) # Move the model to the correct device\n",
        "logits, loss = model(xb, yb)\n",
        "\n",
        "\n",
        "print(decode(model.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=100)[0].tolist()))"
      ],
      "metadata": {
        "id": "BJ2fAr_n2wA0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "20a49eca-6e23-4db1-b9b4-b0eb62b11648"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "WGkwwRaRMowWaCo’R.u\n",
            "Wd hes\n",
            "ed.sW.]Ebp cLpxH,TaWfcsopl[fx,lh’TCG,it,H.GRdOldlR.TxoHMI et’Wn\n",
            "i’ ,p,[iw\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3) # adam optimizer"
      ],
      "metadata": {
        "id": "aHmr7q_2WxIb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training Loop + Output\n",
        "\n",
        "This training loop is like the culmination of everything we've built.\n",
        "\n",
        "It generates batches of data, passes that into the model, the logits and loss are calculated and the test + train loss is printed. This is essential because if we notice the train loss going up but the test loss going down, it means the model is overfitting.\n",
        "\n",
        "It also used backpropagation to calculate learning signals (gradients) for every single parameter in the model, from the embedding tables to the attention weights to the final output layer.\n",
        "\n",
        "Then, the model finally generates max_tokens amount of tokens which is decoded using our decoder and printed onto the terminal."
      ],
      "metadata": {
        "id": "5HAzfkSSjCSJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for iter in range(max_iters):\n",
        "\n",
        "    # every once in a while evaluate the loss on train and val sets\n",
        "    if iter % eval_interval == 0:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "# generate from the model\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "print(decode(model.generate(context, max_new_tokens=1000)[0].tolist()))"
      ],
      "metadata": {
        "id": "6SOe0P6wZB2o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "41678c69-367a-4bd4-a8c3-24c37be566de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 0: train loss 3.9081, val loss 3.9081\n",
            "step 500: train loss 0.1609, val loss 0.1631\n",
            "step 1000: train loss 0.0952, val loss 0.0955\n",
            "step 1500: train loss 0.0845, val loss 0.0830\n",
            "step 2000: train loss 0.0784, val loss 0.0796\n",
            "step 2500: train loss 0.0778, val loss 0.0774\n",
            "step 3000: train loss 0.0751, val loss 0.0750\n",
            "step 3500: train loss 0.0745, val loss 0.0747\n",
            "step 4000: train loss 0.0746, val loss 0.0754\n",
            "step 4500: train loss 0.0737, val loss 0.0738\n",
            "step 5000: train loss 0.0737, val loss 0.0750\n",
            "step 5500: train loss 0.0731, val loss 0.0739\n",
            "step 6000: train loss 0.0724, val loss 0.0738\n",
            "step 6500: train loss 0.0733, val loss 0.0731\n",
            "step 7000: train loss 0.0727, val loss 0.0728\n",
            "step 7500: train loss 0.0726, val loss 0.0726\n",
            "step 8000: train loss 0.0711, val loss 0.0714\n",
            "step 8500: train loss 0.0714, val loss 0.0714\n",
            "step 9000: train loss 0.0709, val loss 0.0702\n",
            "step 9500: train loss 0.0708, val loss 0.0714\n",
            "\n",
            "\n",
            "Greetings, esteemed delegate.\n",
            "We propose a resolution that ensures stability and cooperation.\n",
            "Our delegation aligns itself with the views expressed by [Country].\n",
            "We must rise above differences and work for the common good.\n",
            "\n",
            "Distinguished delegates, I extend my warm regards.\n",
            "My delegation firmly believes in the importance of cooperation.\n",
            "Our delegation cannot support this course of action.\n",
            "Thank you, Honourable Chair, and esteemed delegates.\n",
            "\n",
            "Esteemed delegates, allow me to share my country’s perspective.\n",
            "Our nation advocates for peaceful dialogue and multilateral solutions.\n",
            "We urge the committee to reconsider such a stance.\n",
            "Thank you, Honourable Chair, and esteemed delegates.\n",
            "\n",
            "Esteemed delegates, allow me to share my country’s perspective.\n",
            "We urge the committee to take immediate action on this pressing matter.\n",
            "We stand in solidarity with those advocating for peace.\n",
            "Thank you, Honourable Chair.\n",
            "We urge the committee to take immediate action on this pressing matter.\n",
            "We fully support the\n"
          ]
        }
      ]
    }
  ]
}